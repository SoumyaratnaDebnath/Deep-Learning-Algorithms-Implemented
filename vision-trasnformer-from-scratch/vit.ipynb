{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pdb\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "_device_ = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border:none;\">\n",
    "    <tr style=\"border:none;\">\n",
    "        <td style=\"border:none;\">\n",
    "            <h3>Image 1: Architecture of ViT</h3>\n",
    "            <img src=\"_images_/vit.png\" alt=\"drawing\" width=\"600px\"/>\n",
    "        </td>\n",
    "        <td style=\"border:none;\">\n",
    "            <h3>Image 2: Architecture of ViT for Classification</h3>\n",
    "            <img src=\"_images_/vit_for_classification.png\" alt=\"drawing\" width=\"600px\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of **patch embedding** is to divide the images into patches and then later flatten them to a one dimensional vector before feeding it to the model. Traditionally tramsformers are made to work with sequence of words, thus patch embedding allow us to sequencify the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_embedding(images, n_patch = 7): # n_patch = 7 means 7x7 grid of patches, so there will be 49 patches (note, image size is 28x28)\n",
    "    n, c, h, w = images.shape # for MNIST, n = n (number of images), c = 1 (channels), h = 28 (height), w = 28 (width)\n",
    "\n",
    "    assert h == w, 'Input image should be square'\n",
    "    \n",
    "    patches = torch.zeros(n, n_patch ** 2, h * w * c // n_patch ** 2)   # (N, 49, 16) holds all the patches for all the images\n",
    "                                                                        # 28 * 28 * 1 / 7 * 7 = 16 pixels per patch\n",
    "\n",
    "    patch_size = h // n_patch # 28 / 7 = 4\n",
    "\n",
    "    for idx, image in enumerate(images): # ennumerate over the images\n",
    "        for i in range(n_patch): # iterate over the patches\n",
    "            for j in range(n_patch): # iterate over the patches\n",
    "                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size] # get the patch, (1, 4, 4)\n",
    "                patches[idx, i * n_patch + j] = patch.flatten() # flatten the patch and assign it to the patches tensor\n",
    "\n",
    "    return patches # N x 49 x 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border:none;\">\n",
    "    <tr style=\"border:none;\">\n",
    "        <td style=\"border:none;\">\n",
    "            <h3>Image 3: Equation for posistional encodeing</h3>\n",
    "            <img src=\"_images_/positional_embedding.png\" alt=\"drawing\" width=\"600px\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(sequence_length, d): # 50, 8\n",
    "    \n",
    "    result = torch.ones(sequence_length, d) # 50 x 8\n",
    "\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            # the equation comes form the paper, depicted in the Image 3\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** (j / d)))\n",
    "\n",
    "    return result # 50 x 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d, n_heads = 2): # we have 8 dimensional vectors and 2 heads\n",
    "        super().__init__()\n",
    "\n",
    "        self.d = d # d is the dimension\n",
    "        self.n_heads = n_heads # n_heads is the number of heads\n",
    "\n",
    "        assert d % n_heads == 0, 'd should be divisible by n_heads'\n",
    "\n",
    "        self.d_head = d // n_heads # 8 / 2 = 4\n",
    "\n",
    "        self.q = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(self.n_heads)]) # [(4, 4), (4, 4)] => multi-heads\n",
    "        self.k = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(self.n_heads)]) # [(4, 4), (4, 4)] => multi-heads\n",
    "        self.v = nn.ModuleList([nn.Linear(self.d_head, self.d_head) for _ in range(self.n_heads)]) # [(4, 4), (4, 4)] => multi-heads\n",
    "\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # N, seq_len, token_dim => N, 50, 8\n",
    "        # Each patch will have its own k, q, v\n",
    "        # Now each patch has a dimension 8, and we apply them across two heads, thus we do it twice with 4 dimensions each\n",
    "        results = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads): # 0, 1, or 2 (assuming n_heads is 3 for this example)\n",
    "                q_mapping = self.q[head]\n",
    "                k_mapping = self.k[head]\n",
    "                v_mapping = self.v[head]\n",
    "\n",
    "                # We take the first d_head dimensions and the next d_head dimensions in the next iteration\n",
    "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head] # [0: 4], [4: 8]\n",
    "\n",
    "                # We multiply the sequence with the q, k, v matrices\n",
    "                q = q_mapping(seq)\n",
    "                k = k_mapping(seq)\n",
    "                v = v_mapping(seq)\n",
    "                \n",
    "                # Compute attention scores\n",
    "                attention = self.softmax(torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_head)) # (batch_size, seq_len, seq_len)\n",
    "\n",
    "                # Apply attention to the value matrix\n",
    "                attended_seq = torch.matmul(attention, v) # (batch_size, seq_len, d_head)\n",
    "\n",
    "                seq_result.append(attended_seq) # append the result of the current head\n",
    "\n",
    "            # Concatenate the results of the heads along the last dimension\n",
    "            results.append(torch.hstack(seq_result)) # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # Concatenate the results of the sequences along the batch dimension\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in results], dim=0) # (total_sequences, batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderVIT(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # here we are following the flow mentioned in image 1, norm -> mha -> norm -> mlp\n",
    "        self.norm1 = nn.LayerNorm(hidden_d) \n",
    "        self.mha = MultiHeadAttention(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, hidden_d * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_d * mlp_ratio, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.norm1(x) # 32 x 50 x 8\n",
    "        out1 = self.mha(out1) # 32 x 50 x 8\n",
    "        out1 = x + out1 # adding the x will do the residual connection as shown in image 1 # 32 x 50 x 8\n",
    "        out2 = self.norm2(out1)\n",
    "        out2 = self.mlp(out2)\n",
    "        out2 = out1 + out2\n",
    "\n",
    "        # # A compact way to fo the exact same thing is given below\n",
    "        # out = x + self.mha(self.norm1(x))\n",
    "        # out = out + self.mlp(self.norm2(out))\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "    def __init__(self, channel = 1, height = 28, width = 28, n_patch = 7, n_blocks = 2, hidden_d = 8, n_heads = 2, out_dim = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channel = channel\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.n_patch = n_patch\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert height == width, 'Input image should be square'\n",
    "        assert height % n_patch == 0, 'n_patch should be a factor of height'\n",
    "\n",
    "        self.patch_size = (height // n_patch, width // n_patch) # (4, 4)\n",
    "\n",
    "        # linear mapping of the patches\n",
    "        self.input_d = int(channel * self.patch_size[0] * self.patch_size[1]) # 1 * 4 * 4 = 16\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d) # 16 -> 8\n",
    "\n",
    "        # learnable class token\n",
    "        self.class_token = nn.Parameter(torch.randn(1, self.hidden_d)) # 1 x 8\n",
    "\n",
    "        # positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.zeros((self.n_patch ** 2 + 1, self.hidden_d))) # 50 x 8\n",
    "        self.pos_embed.requires_grad = False\n",
    "\n",
    "        # In image 1, we see the encoder block is repeated n times, here we are repeating it 2 times\n",
    "        # Encoder block\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderVIT(self.hidden_d, self.n_heads) for _ in range(self.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # we just need a final MLP layer to get the output\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_dim),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    " \n",
    "    def forward(self, images):\n",
    "        n, c, h, w = images.shape\n",
    "        # we get the patches\n",
    "        patches = patch_embedding(images, self.n_patch) # N x 49 x 16\n",
    "\n",
    "        # in image 1, we see the patches get passed through a linear projection which outputs an eight dimnsional vector, the next line does the exact same thing\n",
    "        # performing a linear projection on the patches\n",
    "        tokens = self.linear_mapper(patches.to(device=_device_)) # N x 49 x 8\n",
    "\n",
    "        # in image 2, we see along with the patches there is a <cls> token which is passed with each image, thus for each image we need to append a classification\n",
    "        # token, the next line does the exact same thing\n",
    "        # adding the class token to the tokens\n",
    "        tokens = torch.stack([torch.vstack((self.class_token, token)) for token in tokens]) # N x 50 x 8\n",
    "\n",
    "        # in image 2, we see the positional encoding is added to the tokens, the next line does the exact same thing\n",
    "        # adding the positional embedding to the tokens\n",
    "        pos_embed = self.pos_embed.repeat(n, 1, 1) # N x 50 x 8\n",
    "\n",
    "        # in image 1, we see the tokens and positional encoding are added together, the next line does the exact same thing\n",
    "        out = tokens + pos_embed\n",
    "        # transformer block\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # apply the final MLP layer only in the classification token, for that we need to extract the classification token\n",
    "        cls_token = out[:, 0] # 1 x 8\n",
    "        \n",
    "        return self.mlp(cls_token) # 1 x 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since everything is set, we define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    transform = ToTensor()\n",
    "    train_set = MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "    test_set = MNIST(root = './data', train = False, download = True, transform = transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\n",
    "    test_loader = DataLoader(test_set, batch_size = 32, shuffle = False)\n",
    "\n",
    "    device = _device_\n",
    "    model = VIT(channel=1, height=28, width=28, n_patch=7, n_blocks=2, hidden_d=8, n_heads=2, out_dim=10).to(device)\n",
    "    n_epochs = 2\n",
    "    lr = 0.005\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{n_epochs} - Training\"):\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            y_hat = model(images)\n",
    "            loss = criterion(y_hat, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} - Training loss: {train_loss}\")\n",
    "\n",
    "    # testing loop\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in tqdm(test_loader, desc=f\"Epoch {epoch + 1}/{n_epochs} - Testing\"):\n",
    "            images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            y_hat = model(images)\n",
    "            _, predicted = torch.max(y_hat, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs} - Testing accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training: 100%|██████████| 1875/1875 [04:29<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Training loss: 2.1409480080922405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training: 100%|██████████| 1875/1875 [04:30<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training loss: 2.093113217480978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Testing: 100%|██████████| 313/313 [00:21<00:00, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Testing accuracy: 0.3824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
